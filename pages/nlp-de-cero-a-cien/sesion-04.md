---
title: "Sesión 4: Transformers y Aprendizaje por Transferencia"
date: 2021-08-24T16:00:00.000+00:00
lang: es
duration: 66min
author: Lewis Tunstall
---

<div>
    <CourseSummary
        description="Continuamos hablando de Transformers, esta semana explicaremos qué es el aprendizaje por transferencia y el modelado del lenguaje, y cómo utilizar fine-tuning para adaptar modelos preentrenados a nuestro caso de uso particular."
        video="https://www.youtube.com/embed/XeFDEokiK3c"
        slides="https://github.com/somosnlp/nlp-de-cero-a-cien/blob/main/4_transformers_aprendizaje_por_transferencia/aprendizaje_por_transferencia.pdf"
        name="Lewis Tunstall"
        twitter="https://twitter.com/_lewtun"
        linkedin="https://www.linkedin.com/in/lewis-tunstall"
        github="https://github.com/lewtun"
    />
</div>

---

## Notebook

<a href="https://colab.research.google.com/drive/17630ohLuzpQ3jJRp1YSb-05fcbi8STql
" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## Recursos adicionales

* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/): guía ilustrada que explica de manera muy intuitiva los conceptos de aprendizaje por transferencia y clasificación de textos.
* [A Visual Guide to Using BERT for the First Time](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/): guía ilustrada centrada en el uso de BERT como extractor de "features" para la clasificación de textos.
* [Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/): lista de todas las formas de fine-tuning que se realizan hoy en día.

## Papers

* [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146): paper del primer ejemplo en el que el aprendizaje por transferencia en NLP realmente funcionó. Basado en LSTMs y parte de fast.ai
* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf): paper de GPT en el que se utilizó por primera vez el aprendizaje por transferencia con la arquitectura Transformer.
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805?source=post_page): paper de BERT donde se estableció un nuevo state-of-the-art en varias tareas de NLP.

---

➡️ Siguiente sesión: [Transformers y Generación de Texto](/nlp-de-cero-a-cien/sesion-05)
