---
title: Contribute to the INCLUDE benchmark
description: Help us create diverse benchmarks to ensure fair representation in LLMs!
date: 2025-01-09T16:00:00.000+00:00
lang: en
duration: 1min
cover: "https://github.com/somosnlp/assets/raw/main/images/patrocinios/INCLUDE_post.png"
author: Angelika Romanou
bio: PhD Candidate at EPFL (Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne)
website: https://agromanou.github.io/
twitter: https://x.com/agromanou
linkedin: https://www.linkedin.com/in/angelika-romanou-67067057/
---

TL;DR: We are looking for multiple-choice tests in any language to add to the multicultural INCLUDE benchmark. Fill out the [form](https://forms.gle/9VbDMSHvhqSQbtL56) now!

<div class="flex justify-center">
    <img src="https://github.com/somosnlp/assets/raw/main/images/patrocinios/INCLUDE_post.png" alt="INCLUDE" width="400">
</div>

It is often said that large language models (LLMs) are developing increasingly advanced multilingual capabilities, but in which languages exactly? In the case of languages spoken in several countries, which varieties of the language are being considered? If we aim for LLMs to be multilingual and multicultural, we need to ensure that the entire global community is represented in the datasets. Change starts by creating diverse evaluation benchmarks that allow us to measure the current state and make progress toward truly inclusive LLMs.

[INCLUDE](https://arxiv.org/abs/2411.19799) is a multilingual LLM evaluation benchmark with a cultural focus. The first version, published in December 2024, included 42 languages. We are looking for tests in all languages to extend this benchmark and make it more representative!

## ðŸ’¡ Frequently Asked Questions

- **What are the requirements for the exams?** Only one, the answers must be available.
- **Are there any preferences?** We are especially interested in multiple-choice tests. Moreover, the collection campaign is mainly focused on content related to a country's or region's culture (e.g., history, literature). Anyway, we welcome ANY type of test.
- **How can I help?** Send us links to tests via this [form](https://forms.gle/9VbDMSHvhqSQbtL56), we will take care of processing them.  
- **Can I participate more actively in this campaign?** If you are interested, for example, in collecting tests in your country, assisting with processing the exams, or analyzing the results, indicate so in the last question of the form and we will contact you.  
- **How is active collaboration rewarded?** People who process more than 300 questions will be eligible to become co-authors of papers related to the benchmark.

## âœ… Examples of Multiple-Choice Tests

- University access exams  
- School or high school exams  
- Licensing exams for professions (medicine, psychology, law, etc.)  
- Language tests  
- Driving license tests  
- Questions from shows like "Who Wants to Be a Millionaire?"  
- Questions from games like Trivial Pursuit  
- Self-assessment tests in textbooks 

## ðŸ“š Useful Links

- [Original INCLUDE paper](https://arxiv.org/abs/2411.19799)  
- [Dataset on Hugging Face](https://huggingface.co/datasets/CohereForAI/include-base-44)  
- [Form to submit exams](https://forms.gle/9VbDMSHvhqSQbtL56)  

## ðŸš€ Meet the team

Angelika Romanou, Antoine Bosselut, Negar Foroutan and Anna Sotnikova from EPFL (Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne), MarÃ­a Grandury from SomosNLP, Jabez Magomere from the University of Oxford, and Shamsuddeen H. Muhammad from Imperial College London.

## ðŸ‘‹ Contact

- If you indicate in the form that you are interested in participating actively, we will contact you via email!
